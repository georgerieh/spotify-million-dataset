{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgerieh/spotify-million-dataset/blob/main/Spotify_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj0RjduRXCLA"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "from cupyx.scipy.sparse import csr_matrix as cp_csr_matrix\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.sparse import csr_matrix as sp_csr_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kehswSXOYhhp"
      },
      "source": [
        "# Plan\n",
        "## Part 1\n",
        "Clean features, check variance, distribution and missing values\n",
        "## Part 2\n",
        "Compute correlations of continous features with followers, test categorical features with ANOVA/Kruskall-Wallis\n",
        "## Part 3\n",
        "Check redundancy between features\n",
        "\n",
        "## Part 4\n",
        "Fit a simple model to gauge feature importance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mX0eb7eY-1b"
      },
      "source": [
        "## Part 0\n",
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232KNxRsZ-IK",
        "outputId": "67177f31-c942-4cde-c428-f90dec38dca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knpM6WzSZ40n"
      },
      "outputs": [],
      "source": [
        "# !unzip '/content/drive/MyDrive/spotify_million_playlist_dataset.zip' -d 'spotify_million_playlist_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKlc8hvckA8a",
        "outputId": "382607c0-a08a-4eb7-9b6a-53159a41d7bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [11:04<00:00,  1.51it/s]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import glob\n",
        "import csv\n",
        "import hashlib\n",
        "import re\n",
        "\n",
        "def stable_hash(s):\n",
        "    return int(hashlib.md5(s.encode('utf-8')).hexdigest(), 16)\n",
        "\n",
        "def process_and_split_data():\n",
        "    files = glob.glob(\"spotify_million_playlist_dataset/data/*.json\")\n",
        "    seen_songs = set()\n",
        "\n",
        "    with open('/content/drive/MyDrive/spotify-challenge/interactions.csv', \"w\", newline=\"\") as f_int, \\\n",
        "         open('/content/drive/MyDrive/spotify-challenge/song_library.csv', \"w\", newline=\"\") as f_song, \\\n",
        "         open('/content/drive/MyDrive/spotify-challenge/playlist_library.csv', \"w\", newline=\"\") as f_play:\n",
        "\n",
        "        int_writer = csv.writer(f_int)\n",
        "        song_writer = csv.writer(f_song)\n",
        "        play_writer = csv.writer(f_play)\n",
        "\n",
        "        # Headers\n",
        "        int_writer.writerow([\"playlist_id\", \"track_hash\", 'added', \"pos\"])\n",
        "        song_writer.writerow([\"track_hash\", \"name\", \"artist\", \"album\", \"dur\"])\n",
        "        play_writer.writerow([\"pid\", \"p_name\", \"is_shared\", \"n_tracks\", \"n_followers\", 'n_albums'])\n",
        "\n",
        "        for file_name in tqdm(files):\n",
        "            with open(file_name, \"r\") as jf:\n",
        "                data = json.load(jf)\n",
        "                for playlist in data[\"playlists\"]:\n",
        "                    pid = playlist[\"pid\"]\n",
        "\n",
        "                    # Store Playlist Metadata ONCE per playlist\n",
        "                    play_writer.writerow([\n",
        "                        pid, playlist['name'], bool(playlist['collaborative']),\n",
        "                        playlist['num_tracks'], playlist['num_followers'], playlist['num_albums']\n",
        "                    ])\n",
        "\n",
        "                    for track in playlist[\"tracks\"]:\n",
        "                        # tid = track['track_uri']\n",
        "                        track_hash = stable_hash(re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", lower(track['track_name']+track['album_name'])))\n",
        "\n",
        "                        # Store Interaction (The heavy file, but only IDs/numbers)\n",
        "                        int_writer.writerow([pid, track_hash,1,track['pos']])\n",
        "\n",
        "                        # Store Song Metadata ONCE per unique song\n",
        "                        if tid not in seen_songs:\n",
        "                            song_writer.writerow([\n",
        "                                track_hash, track['track_name'], track['artist_name'],\n",
        "                                track['album_name'], track['duration_ms']\n",
        "                            ])\n",
        "                            seen_songs.add(track_hash)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gES8WGGptAHe",
        "outputId": "32c47c64-a1b1-456b-d5f0-9a9bcdfba80a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "67it [03:34,  3.20s/it]\n"
          ]
        }
      ],
      "source": [
        "playlist_map = {}\n",
        "song_map = {}\n",
        "\n",
        "next_playlist_idx = 0\n",
        "next_song_idx = 0\n",
        "\n",
        "for chunk in tqdm(pd.read_csv(\"/content/drive/MyDrive/spotify-challenge/interactions.csv\", chunksize=1_000_000)):\n",
        "    for row in chunk.itertuples():\n",
        "        pid_hash = int(row.playlist_id)\n",
        "        sid_hash = int(row.track_hash)\n",
        "        if pid_hash not in playlist_map:\n",
        "            playlist_map[int(pid_hash)] = next_playlist_idx\n",
        "            next_playlist_idx += 1\n",
        "        if sid_hash not in song_map:\n",
        "            song_map[int(sid_hash)] = next_song_idx\n",
        "            next_song_idx += 1\n",
        "\n",
        "with open('/content/drive/MyDrive/spotify-challenge/playlist_map.json', 'w') as pl, open('/content/drive/MyDrive/spotify-challenge/song_map.json', 'w') as so,\n",
        "  json.dump(playlist_map, pl)\n",
        "  json.dump(song_map, so)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFaa6kVMueHW",
        "outputId": "bf6e21ef-b5fb-44f8-f511-3b1be8f075e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "67it [03:44,  3.35s/it]\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import coo_matrix, save_npz, load_npz\n",
        "\n",
        "rows = []\n",
        "cols = []\n",
        "data_vals = []\n",
        "\n",
        "for chunk in tqdm(pd.read_csv(\"/content/drive/MyDrive/spotify-challenge/interactions.csv\", chunksize=1_000_000)):\n",
        "    for row in chunk.itertuples():\n",
        "        pid_idx = playlist_map[int(row.playlist_id)]\n",
        "        sid_idx = song_map[int(row.song_id)]\n",
        "\n",
        "        rows.append(pid_idx)\n",
        "        cols.append(sid_idx)\n",
        "        data_vals.append(1)  # binary interaction\n",
        "\n",
        "interaction_matrix = coo_matrix(\n",
        "    (data_vals, (rows, cols)),\n",
        "    shape=(len(playlist_map), len(song_map))\n",
        ")\n",
        "\n",
        "save_npz('/content/drive/MyDrive/spotify-challenge/interaction.npz', interaction_matrix)\n",
        "del interaction_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qfd5MXubyZ69"
      },
      "outputs": [],
      "source": [
        "interaction_matrix = load_npz('/content/drive/MyDrive/spotify-challenge/interaction_npz')\n",
        "interaction_matrix = interaction_matrix.tocsr()\n",
        "save_npz('/content/drive/MyDrive/spotify-challenge/interaction_csr.npz', interaction_matrix)\n",
        "del interaction_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scOODOvBfQ6-"
      },
      "outputs": [],
      "source": [
        "all_recommendations = {}  # key = playlist index, value = list of top-n songs\n",
        "\n",
        "def save_recommendations(playlist_idx, recommended_songs):\n",
        "    all_recommendations[playlist_idx] = recommended_songs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV7KtxWwP9Or"
      },
      "outputs": [],
      "source": [
        "\n",
        "def csr_to_torch_sparse(csr_matrix):\n",
        "    coo = csr_matrix.tocoo()  # must be COO\n",
        "    if coo.data.size == 0:\n",
        "        # empty matrix → return a 0-valued sparse tensor\n",
        "        indices = torch.zeros((2,0), dtype=torch.long)\n",
        "        values  = torch.zeros((0,), dtype=torch.float32)\n",
        "    else:\n",
        "        indices = torch.tensor([coo.row, coo.col], dtype=torch.long)\n",
        "        values  = torch.tensor(coo.data, dtype=torch.float32)  # must be float\n",
        "    shape = coo.shape\n",
        "    return torch.sparse_coo_tensor(indices, values, size=shape)\n",
        "\n",
        "def compute_topk_similarity_gpu(\n",
        "    sparse_x_batch_cpu: sp_csr_matrix,\n",
        "    csr_matrix_all_cupy: cp_csr_matrix,\n",
        "    top_k: int = 5,\n",
        "    zero_self_similarity: bool = False,\n",
        "    batch_offset: int = 0,\n",
        "):\n",
        "    # Convert CPU batch to GPU (CuPy)\n",
        "    sparse_x_batch_gpu = cp_csr_matrix(sparse_x_batch_cpu)\n",
        "\n",
        "    # Calculate L2 norms for normalization\n",
        "    # Example: If a row is [3, 0, 4], power(2) is [9, 0, 16], sum is 25, sqrt is 5.\n",
        "    all_norms = cp.sqrt(csr_matrix_all_cupy.power(2).sum(axis=1)).ravel()\n",
        "    batch_norms = cp.sqrt(sparse_x_batch_gpu.power(2).sum(axis=1)).ravel()\n",
        "\n",
        "    # Compute sparse dot product (Batch Matrix Multiplication)\n",
        "    # ====== #\n",
        "    # sparse_x_batch (2 rows) = [[1, 0], [0, 1]]\n",
        "    # csr_matrix_all (3 rows) = [[1, 1], [1, 0], [0, 1]]\n",
        "    # sim = batch @ all.T\n",
        "    # sim result shape: (2, 3) -> Matrix of dot products\n",
        "    # sim = [[1, 1, 0],\n",
        "    #        [1, 0, 1]]\n",
        "    # ====== #\n",
        "    sim = sparse_x_batch_gpu @ csr_matrix_all_cupy.T\n",
        "\n",
        "    if cp.sparse.issparse(sim):\n",
        "        sim = sim.toarray().astype('f')\n",
        "\n",
        "    # Normalize similarity scores\n",
        "    # ====== #\n",
        "    # batch_norms = [1.0, 1.0] (shape 2,)\n",
        "    # all_norms = [1.41, 1.0, 1.0] (shape 3,)\n",
        "    # batch_norms[:, None] transforms to (2, 1) column\n",
        "    # all_norms[None, :] transforms to (1, 3) row\n",
        "    # Multiplying them creates a (2, 3) denominator matrix:\n",
        "    # [[1.41, 1.0, 1.0],\n",
        "    #  [1.41, 1.0, 1.0]]\n",
        "    # sim = sim / (denominator + 1e-8)\n",
        "    # ====== #\n",
        "    sim = sim / (batch_norms[:, None] * all_norms[None, :] + 1e-8)\n",
        "\n",
        "    # Optional: remove self-similarity\n",
        "    if zero_self_similarity:\n",
        "        rows_in_batch = sparse_x_batch_gpu.shape[0]\n",
        "        for i in tqdm(range(rows_in_batch)):\n",
        "            global_idx = batch_offset + i\n",
        "            if global_idx < csr_matrix_all_cupy.shape[0]:\n",
        "                # Sets the diagonal element (the item vs itself) to 0\n",
        "                sim[i, global_idx] = 0.0\n",
        "\n",
        "    # Top-k selection (GPU)\n",
        "    # ====== #\n",
        "    # sim = [[0.7, 1.0, 0.2],\n",
        "    #        [0.1, 0.5, 0.9]]\n",
        "    # cp.argsort(sim, axis=1) ->\n",
        "     #[[2, 0, 1],\n",
        "     #[0, 1, 2]] (indices of values low to high)\n",
        "    # [:, ::-1] reverses it    -> [[1, 0, 2], [2, 1, 0]] (indices of values high to low)\n",
        "    # [:, :top_k] (top_k=2)    -> [[1, 0], [2, 1]]\n",
        "    # ====== #\n",
        "    topk_idx = cp.argsort(sim, axis=1)[:, ::-1][:, :top_k]\n",
        "\n",
        "    # Use take_along_axis to get the corresponding values\n",
        "    # ====== #\n",
        "    # From sim, pick values at indices [[1, 0], [2, 1]]\n",
        "    # topk_vals = [[1.0, 0.7], [0.9, 0.5]]\n",
        "    # ====== #\n",
        "    topk_vals = cp.take_along_axis(sim, topk_idx, axis=1)\n",
        "\n",
        "    return topk_idx.get(), topk_vals.get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6jOxdCnINceo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "from cupy.sparse import csr_matrix as cp_csr_matrix\n",
        "from scipy.sparse import csr_matrix as sp_csr_matrix\n",
        "\n",
        "# Convert the interaction_matrix to CuPy's sparse matrix format once\n",
        "# This will be used for GPU computations.\n",
        "# The original scipy.sparse.csr_matrix is still named 'interaction_matrix'\n",
        "# and will be used by functions like gather_candidate_songs and score_candidate_songs\n",
        "# that are not yet cupy-enabled.\n",
        "interaction_matrix = load_npz('/content/drive/MyDrive/spotify-challenge/interaction_csr.npz')\n",
        "interaction_matrix_gpu = cp_csr_matrix(interaction_matrix.astype('f'))\n",
        "def score_candidate_songs(candidate_songs_indices, top_similar_playlist_indices, top_similar_scores, interaction_matrix):\n",
        "    scores = {}\n",
        "    for song_idx in candidate_songs_indices:\n",
        "        score = 0.0\n",
        "        for i, sim_playlist_idx in enumerate(top_similar_playlist_indices):\n",
        "            # Check if the song is in the similar playlist\n",
        "            if interaction_matrix[sim_playlist_idx, song_idx] > 0:\n",
        "                score += top_similar_scores[i]  # Add the similarity score\n",
        "        scores[song_idx] = score\n",
        "    return scores\n",
        "# Redefine the compute_topk_similarity_gpu function to use CuPy\n",
        "\n",
        "import cupy as cp\n",
        "from cupyx.scipy.sparse import csr_matrix as cp_csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "# 1. SETUP\n",
        "interaction_matrix = load_npz('interaction_csr.npz').astype('f')\n",
        "interaction_matrix_gpu = cp_csr_matrix(interaction_matrix)\n",
        "\n",
        "batch_size = 512\n",
        "top_k = 5\n",
        "top_n = 500  # Based on your goal of 500 songs\n",
        "\n",
        "for batch_start in tqdm(range(0, interaction_matrix_gpu.shape[0], batch_size)):\n",
        "    batch_end = min(batch_start + batch_size, interaction_matrix_gpu.shape[0])\n",
        "    target_batch_cpu = interaction_matrix[batch_start:batch_end, :]\n",
        "\n",
        "    # [DATA MOVE] CPU RAM -> GPU VRAM\n",
        "    # Shape: (batch_size, num_songs) -> [████████] (Sparse)\n",
        "    target_batch_gpu = cp_csr_matrix(target_batch_cpu.astype('f'))\n",
        "\n",
        "    # [SIMILARITY] Finds Top 5 similar playlists for each of the 50 playlists in the batch\n",
        "    # Output Indices: (50, 5) -> [[P1, P2, P3, P4, P5], ...]\n",
        "    # Output Scores:  (50, 5) -> [[.9, .8, .7, .6, .5], ...]\n",
        "    topk_indices, topk_scores = compute_topk_similarity_gpu(\n",
        "        target_batch_cpu,\n",
        "        interaction_matrix_gpu,\n",
        "        top_k=5,\n",
        "        batch_offset=batch_start\n",
        "    )\n",
        "\n",
        "    # [FLATTEN] (50, 5) -> (250,) | 1D list of all neighbor IDs to grab them at once\n",
        "    neighbor_indices = cp.array(topk_indices.ravel())\n",
        "\n",
        "    # [GATHER] Extraction from the main 1M x 2M matrix\n",
        "    # (250,) indices + (1M, 2M) Matrix -> (250, 2M) Neighbor Matrix\n",
        "    # Visual: [Batch1_Neighbors]\n",
        "    #         [Batch2_Neighbors]\n",
        "    neighbor_matrix = interaction_matrix_gpu[neighbor_indices]\n",
        "\n",
        "    # [WEIGHTS] (50, 5) -> (250, 1) | Vertical column of similarity scores\n",
        "    weights = cp.array(topk_scores).reshape(-1, 1)\n",
        "\n",
        "    # [SCALING] Sparse Row * Weight Scalar\n",
        "    # Each song in Neighbor_1 is multiplied by Similarity_1\n",
        "    # Shape stays: (250, 2M)\n",
        "    weighted_neighbors = neighbor_matrix.multiply(weights)\n",
        "\n",
        "    num_songs = interaction_matrix_gpu.shape[1]\n",
        "    current_batch_size = batch_end - batch_start\n",
        "\n",
        "    # [EXPANSION] Sparse -> Dense 3D Tensor (The \"Monster\" Step)\n",
        "    # (250, 2M) becomes (50, 5, 2M)\n",
        "    # Visual: A 3D cube where Depth=5 neighbors, Height=50 playlists, Width=2M songs\n",
        "    # [ [ [S1, S2...], [S1, S2...] (5 of these) ], (50 of these) ]\n",
        "    weighted_dense = weighted_neighbors.toarray().reshape(current_batch_size, top_k, num_songs)\n",
        "\n",
        "    # [COLLAPSE/SUM] Summing through the \"Depth\" axis\n",
        "    # (50, 5, 2M) -> (50, 2M)\n",
        "    # The scores of all 5 neighbors are merged into 1 score-row per playlist\n",
        "    batch_scores = weighted_dense.sum(axis=1)\n",
        "\n",
        "    # [COORDINATE MAPPING] Finding \"Already Listened\" songs\n",
        "    # Target_COO gives: (row_index, col_index) for every '1' in the user's history\n",
        "    target_coo = target_batch_gpu.tocoo()\n",
        "    rows = target_coo.row\n",
        "    cols = target_coo.col\n",
        "\n",
        "    # [MASKING] Set scores to -infinity so they aren't recommended\n",
        "    # batch_scores[row, col] -> Value becomes -100,000,000\n",
        "    batch_scores[rows, cols] = -1e8\n",
        "\n",
        "    # [SORTING] Finding the 10 highest numbers in a row of 2,000,000\n",
        "    # Shape: (50, 2M) -> (50, 10) | Only Song IDs remain\n",
        "    recommended_indices_gpu = cp.argsort(-batch_scores, axis=1)[:, :top_n]\n",
        "\n",
        "    # [COLLECT] Move small 50x10 result matrix: GPU VRAM -> CPU RAM\n",
        "    final_recommendations = recommended_indices_gpu.get()\n",
        "\n",
        "    # [OUTPUT] Loop through 50 rows to save to disk\n",
        "    for i, recommended_songs in enumerate(final_recommendations):\n",
        "        current_playlist_idx = batch_start + i\n",
        "        save_recommendations(current_playlist_idx, recommended_songs.tolist())\n",
        "\n",
        "    # [PURGE] Clear the \"Cube\" (weighted_dense) from memory to avoid OutOfMemory\n",
        "    cp.get_default_memory_pool().free_all_blocks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzLThUztmalo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titles Embeddings"
      ],
      "metadata": {
        "id": "LIH0Lr1okGur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the most popular industry model for short text\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "def get_context_vector(name):\n",
        "    return nlp_model.encode([name])[0]\n",
        "# 2. Extract your playlist titles\n",
        "# Assuming your titles are in a list called 'playlist_titles'\n",
        "df_playlist = pd.read_csv('/content/drive/My Drive/spotify-challenge/playlist_library.csv')\n",
        "titles = df_playlist['p_name']\n",
        "\n",
        "# 3. Generate Embeddings (Industry Practice: Use Batching)\n",
        "# This turns text into a (1000000, 384) float matrix\n",
        "title_embeddings = model.encode(titles, batch_size=128, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# 4. Move to GPU for fast search later\n",
        "title_embeddings_gpu = cp.array(title_embeddings)\n",
        "\n",
        "# 5. Save to Drive (Checkpointing)\n",
        "cp.save('/content/drive/My Drive/spotify-challenge/title_embeddings.npy', title_embeddings_gpu)"
      ],
      "metadata": {
        "id": "kgbanm4-kGPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA\n"
      ],
      "metadata": {
        "id": "lcza4sYPpbZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 1. Concetration analysis\n",
        "\n",
        "Tiny fraction of songs dominates vast majority of my dataset. We check it to eliminate tracks, if a lot of them are in 1-2-3 playlists only (too niche)\n",
        "\n",
        "1. <1% - use IDF\n",
        "2. 10-20% - CF works well, cold-start - no. Use popularity dampling\n",
        "3. >30%. Matrix too sparse - align with metadata"
      ],
      "metadata": {
        "id": "_elgy3VZr7M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Get the counts of every track occurrence\n",
        "# This counts how many times each track appears across all 1M playlists\n",
        "track_counts = df_playlists['track_ids'].explode().value_counts()\n",
        "total_interactions = track_counts.sum()\n",
        "\n",
        "# 2. Sort counts descending (Popular to Niche)\n",
        "sorted_counts = track_counts.values\n",
        "num_unique_tracks = len(sorted_counts)\n",
        "\n",
        "# 3. Calculate cumulative percentage\n",
        "# This tells us: \"The top X songs account for Y% of all data\"\n",
        "cum_percentage = np.cumsum(sorted_counts) / total_interactions\n",
        "\n",
        "# 4. Find the specific index for the 80% threshold\n",
        "eighty_percent_idx = np.searchsorted(cum_percentage, 0.80)\n",
        "percent_of_library = (eighty_percent_idx / num_unique_tracks) * 100\n",
        "\n",
        "print(f\"Total Unique Tracks: {num_unique_tracks}\")\n",
        "print(f\"Number of songs making up 80% of data: {eighty_percent_idx}\")\n",
        "print(f\"Percentage of Library: {percent_of_library:.2f}%\")"
      ],
      "metadata": {
        "id": "cvwUHxNcpa4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(np.linspace(0, 1, len(cum_percentage)), cum_percentage, label='Your Dataset')\n",
        "plt.plot([0, 1], [0, 1], 'r--', label='Perfect Equality (Every song equal)')\n",
        "plt.title(\"Lorenz Curve: Concentration of Music Plays\")\n",
        "plt.xlabel(\"Cumulative % of Unique Tracks\")\n",
        "plt.ylabel(\"Cumulative % of Total Playlist Placements\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IitqVBGJsmPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis 2. Popularity correlation - Positional Bias\n",
        "\n",
        "We check it to weighten tracks, if the hypothesis is true\n",
        "\n",
        "1. Steep downward slope for first 10 songs. Exponentil decay weightning is solution:\n",
        "$$Value = 1.0 \\times e^{-\\lambda \\cdot pos}$$\n",
        "\n",
        "\n",
        "2.Mostly flat line. Position doesnt matter.\n",
        "\n",
        "3. Albums ranker behavior\n"
      ],
      "metadata": {
        "id": "I_mbyIY7svXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Calculate Global Popularity (How many times each track appears in the whole dataset)\n",
        "track_popularity = df_playlist['track_hash'].value_counts().reset_index()\n",
        "track_popularity.columns = ['track_hash', 'global_count']\n",
        "\n",
        "# 2. Join popularity back to the main table\n",
        "# This adds a 'global_count' column next to every track instance\n",
        "df_with_pop = df_playlist.merge(track_popularity, on='track_hash', how='left')\n",
        "\n",
        "# 3. Filter for the \"Head\" of playlists (e.g., first 50 positions)\n",
        "# This prevents extreme outliers from very long playlists from skewing the mean\n",
        "df_head = df_with_pop[df_with_pop['pos'] <= 50]\n",
        "\n",
        "# 4. Calculate the Mean Global Popularity for each position\n",
        "# This is the \"Magic Moment\": it shows if position 0 is more 'popular' than position 40\n",
        "positional_trend = df_head.groupby('pos')['global_count'].mean()\n",
        "\n",
        "# 5. Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(positional_trend.index, positional_trend.values, color='#1DB954', linewidth=2)\n",
        "plt.fill_between(positional_trend.index, positional_trend.values, alpha=0.2, color='#1DB954')\n",
        "plt.title(\"Is there a Positional Bias? (Avg Popularity vs. Playlist Position)\")\n",
        "plt.xlabel(\"Position in Playlist (0 = First Song)\")\n",
        "plt.ylabel(\"Average Global Popularity\")\n",
        "plt.xticks(range(0, 51, 5))\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UP9lGw2-surP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# 1. Take your EDA trend data\n",
        "# x = positions (0, 1, 2...), y = mean global popularity\n",
        "x_data = positional_trend.index.values\n",
        "y_data = positional_trend.values\n",
        "\n",
        "# 2. Normalize y_data so it starts at 1.0 (to match the e^-λx scale)\n",
        "y_data_norm = y_data / y_data[0]\n",
        "\n",
        "# 3. Define the decay function\n",
        "def decay_func(x, lam):\n",
        "    return np.exp(-lam * x)\n",
        "\n",
        "# 4. Fit the curve to find the optimal lambda\n",
        "popt, _ = curve_fit(decay_func, x_data, y_data_norm, p0=[0.05])\n",
        "best_lam = popt[0]\n",
        "\n",
        "print(f\"The mathematically optimal lambda for your data is: {best_lam:.4f}\")"
      ],
      "metadata": {
        "id": "XlMHvjabvVy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Smn0YcOkvFa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis 3. Cohesion hypothesis\n",
        "\n",
        "We check it, to understand whether we rely on CF or we prioritize album/artists"
      ],
      "metadata": {
        "id": "qs7_J7rww4PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Artist Consistency per Playlist\n",
        "# Using your flat (pid, track_hash) table\n",
        "# 1. Join Artist IDs to your table\n",
        "df_with_meta = df_playlist.merge(track_metadata[['track_hash', 'artist_name', 'album_name']], on='track_hash')\n",
        "\n",
        "# 2. Group by PID and count unique artists vs total tracks\n",
        "cohesion_stats = df_with_meta.groupby('pid').agg(\n",
        "    unique_artists=('artist_id', 'nunique'),\n",
        "    total_tracks=('track_hash', 'count'),\n",
        "    top_artist_ratio=('artist_id', lambda x: x.value_counts().max() / len(x))\n",
        ")\n",
        "\n",
        "# 3. Insight: If top_artist_ratio is high (e.g. > 0.3), users are 'Artist-Loyal': rely on CF only if stats is low\n",
        "print(cohesion_stats['top_artist_ratio'].mean())"
      ],
      "metadata": {
        "id": "fPnLGR3CvFJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis 4. Vocabulary hypothesis\n",
        "\n",
        "We check it to use FAISS or not"
      ],
      "metadata": {
        "id": "0uIMUPJWyB1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_count = len(df)\n",
        "\n",
        "# 1. Raw Uniqueness\n",
        "raw_unique = df[title_col].nunique()\n",
        "\n",
        "# 2. Cleaned Uniqueness (Normalization)\n",
        "# We remove symbols, emojis, and lowercase everything\n",
        "def clean_title(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Keep only alphanumeric\n",
        "    return text.strip()\n",
        "\n",
        "cleaned_series = df[title_col].apply(clean_title)\n",
        "# Remove empty strings after cleaning (like playlists named '!!!')\n",
        "cleaned_series = cleaned_series[cleaned_series != \"\"]\n",
        "\n",
        "clean_unique = cleaned_series.nunique()\n",
        "\n",
        "# 3. Calculate Results\n",
        "print(f\"--- Title Uniqueness Report ---\")\n",
        "print(f\"Total Playlists:         {total_count:,}\")\n",
        "print(f\"Raw Unique Titles:       {raw_unique:,}  ({(raw_unique/total_count)*100:.2f}%)\")\n",
        "print(f\"Cleaned Unique Titles:   {clean_unique:,}  ({(clean_unique/total_count)*100:.2f}%)\")\n",
        "\n",
        "return cleaned_series.value_counts()\n",
        "top_10 = top_titles.head(10)\n",
        "print(\"Top 10 Cleaned Titles:\\n\", top_10)"
      ],
      "metadata": {
        "id": "gLuHU7OYqp_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis 5. Temporal flow\n",
        "\n"
      ],
      "metadata": {
        "id": "AmB4W-lf0E0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Calculate the 'Library Standard Deviation' (The baseline)\n",
        "library_std = df_tracks['duration_ms'].std()\n",
        "\n",
        "# 2. Join durations to your flat playlist table (pid, track_hash)\n",
        "df_with_dur = df.merge(df_tracks[['track_hash', 'duration_ms']], on='track_hash')\n",
        "\n",
        "# 3. Calculate the 'Internal Standard Deviation' for every playlist\n",
        "playlist_stats = df_with_dur.groupby('pid')['duration_ms'].agg(['mean', 'std', 'count'])\n",
        "\n",
        "# 4. Filter out very short playlists (which have artificially low STD)\n",
        "playlist_stats = playlist_stats[playlist_stats['count'] > 5]\n",
        "\n",
        "# 5. Insight: If Avg(Internal STD) < Library STD, users care about pacing\n",
        "print(f\"Library Duration STD: {library_std:.2f}\")\n",
        "print(f\"Avg Playlist Duration STD: {playlist_stats['std'].mean():.2f}\")"
      ],
      "metadata": {
        "id": "VnCp7r2m0Iqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "cycwUMchnXp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_test_set(playlists_df, n_test=1000):\n",
        "    # 1. Randomly sample 1000 playlists\n",
        "    test_samples = playlists_df.sample(n_test)\n",
        "\n",
        "    test_data = []\n",
        "    for idx, row in test_samples.iterrows():\n",
        "        all_tracks = row['track_ids']\n",
        "        if len(all_tracks) < 10: continue # Skip tiny playlists\n",
        "\n",
        "        # 2. Split: Use 5 tracks as \"Input\" and the rest as \"Ground Truth\"\n",
        "        input_tracks = all_tracks[:5]\n",
        "        hidden_tracks = all_tracks[5:]\n",
        "\n",
        "        test_data.append({\n",
        "            'title': row['title'],\n",
        "            'input_tracks': input_tracks,\n",
        "            'hidden_tracks': hidden_tracks\n",
        "        })\n",
        "    return test_data\n",
        "\n",
        "# Usage\n",
        "# my_test_set = create_test_set(original_data_df)"
      ],
      "metadata": {
        "id": "wSpbZQE9nauL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation(test_set, faiss_index, model):\n",
        "    total_recall = 0\n",
        "\n",
        "    for case in test_set:\n",
        "        # STEP A: Create Title Vector\n",
        "        query_vec = model.encode([case['title']])\n",
        "        faiss.normalize_L2(query_vec)\n",
        "\n",
        "        # STEP B: Search for 100 Similar Playlists\n",
        "        # This gives us a \"Neighborhood\"\n",
        "        distances, neighbor_indices = faiss_index.search(query_vec, 100)\n",
        "\n",
        "        # STEP C: Aggregation (The \"Voting\")\n",
        "        # Count which songs appear most in those 100 playlists\n",
        "        candidate_songs = []\n",
        "        for n_idx in neighbor_indices[0]:\n",
        "            candidate_songs.extend(get_songs_for_playlist(n_idx))\n",
        "\n",
        "        # Get the Top 500 most frequent songs\n",
        "        from collections import Counter\n",
        "        top_500_recs = [song for song, count in Counter(candidate_songs).most_common(500)]\n",
        "\n",
        "        # STEP D: Calculate Hit Rate\n",
        "        hits = len(set(top_500_recs).intersection(set(case['hidden_tracks'])))\n",
        "        recall = hits / len(case['hidden_tracks'])\n",
        "        total_recall += recall\n",
        "\n",
        "    avg_recall = total_recall / len(test_set)\n",
        "    print(f\"Average Recall@500 for Title-Only: {avg_recall:.2%}\")"
      ],
      "metadata": {
        "id": "ulFfnRpenXci"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ky9HU4H1oOk3FV1nwoYeSsYcKzVxvGvi",
      "authorship_tag": "ABX9TyMCgfR63LkPHitxsZQSQmPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}